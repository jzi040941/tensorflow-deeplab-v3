{"tensorflow-deeplab-v3":{"deeplab_model.py":[{"name":"_BATCH_NORM_DECAY","type":"v","filepath":"deeplab_model.py","query":"/^_BATCH_NORM_DECAY = 0.9997$/;\"","line":17},{"name":"_WEIGHT_DECAY","type":"v","filepath":"deeplab_model.py","query":"/^_WEIGHT_DECAY = 5e-4$/;\"","line":18},{"name":"atrous_spatial_pyramid_pooling","type":"f","filepath":"deeplab_model.py","query":"/^def atrous_spatial_pyramid_pooling(inputs, output_stride, batch_norm_decay, is_training, depth=256):$/;\"","line":21},{"name":"compute_mean_iou","type":"f","filepath":"deeplab_model.py","query":"/^  def compute_mean_iou(total_cm, name='mean_iou'):$/;\"","class":"function:deeplabv3_model_fn","line":274},{"name":"deeplab_v3_generator","type":"f","filepath":"deeplab_model.py","query":"/^def deeplab_v3_generator(num_classes,$/;\"","line":69},{"name":"deeplabv3_model_fn","type":"f","filepath":"deeplab_model.py","query":"/^def deeplabv3_model_fn(features, labels, mode, params):$/;\"","line":144},{"name":"model","type":"f","filepath":"deeplab_model.py","query":"/^  def model(inputs, is_training):$/;\"","class":"function:deeplab_v3_generator","line":109}],"train.py":[{"name":"_BATCH_NORM_DECAY","type":"v","filepath":"train.py","query":"/^_BATCH_NORM_DECAY = 0.9997$/;\"","line":94},{"name":"_DEPTH","type":"v","filepath":"train.py","query":"/^_DEPTH = 3$/;\"","line":86},{"name":"_HEIGHT","type":"v","filepath":"train.py","query":"/^_HEIGHT = 513$/;\"","line":84},{"name":"_IGNORE_LABEL","type":"v","filepath":"train.py","query":"/^_IGNORE_LABEL = 255$/;\"","line":89},{"name":"_MAX_SCALE","type":"v","filepath":"train.py","query":"/^_MAX_SCALE = 2.0$/;\"","line":88},{"name":"_MIN_SCALE","type":"v","filepath":"train.py","query":"/^_MIN_SCALE = 0.5$/;\"","line":87},{"name":"_MOMENTUM","type":"v","filepath":"train.py","query":"/^_MOMENTUM = 0.9$/;\"","line":92},{"name":"_NUM_CLASSES","type":"v","filepath":"train.py","query":"/^_NUM_CLASSES = 21$/;\"","line":83},{"name":"_NUM_IMAGES","type":"v","filepath":"train.py","query":"/^_NUM_IMAGES = {$/;\"","line":96},{"name":"_POWER","type":"v","filepath":"train.py","query":"/^_POWER = 0.9$/;\"","line":91},{"name":"_WIDTH","type":"v","filepath":"train.py","query":"/^_WIDTH = 513$/;\"","line":85},{"name":"choices","type":"v","filepath":"train.py","query":"/^                    choices=['poly', 'piecewise'],$/;\"","line":45},{"name":"choices","type":"v","filepath":"train.py","query":"/^                    choices=['resnet_v2_50', 'resnet_v2_101'],$/;\"","line":55},{"name":"choices","type":"v","filepath":"train.py","query":"/^                    choices=[8, 16],$/;\"","line":62},{"name":"get_filenames","type":"f","filepath":"train.py","query":"/^def get_filenames(is_training, data_dir):$/;\"","line":102},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Base directory for the model.')$/;\"","line":21},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='End learning rate for the optimizer.')$/;\"","line":72},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Freeze batch normalization parameters during the training.')$/;\"","line":66},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Initial global step for controlling learning rate when fine-tuning model.')$/;\"","line":75},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Initial learning rate for the optimizer.')$/;\"","line":69},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Learning rate policy to optimize loss.')$/;\"","line":46},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Max number of batch elements to generate for Tensorboard.')$/;\"","line":39},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Number of examples per batch.')$/;\"","line":42},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Number of maximum iteration used for \"poly\" learning rate policy.')$/;\"","line":49},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Number of training epochs: '$/;\"","line":27},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Output stride for DeepLab v3. Currently 8 or 16 is supported.')$/;\"","line":63},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Path to the directory containing the PASCAL VOC data tf record.')$/;\"","line":52},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Path to the pre-trained model checkpoint.')$/;\"","line":59},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='The architecture of base Resnet building block.')$/;\"","line":56},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='The number of training epochs to run between evaluations.')$/;\"","line":36},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='The weight decay to use for regularizing the model.')$/;\"","line":78},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Whether to clean up the model directory if present.')$/;\"","line":24},{"name":"help","type":"v","filepath":"train.py","query":"/^                    help='Whether to use debugger to track down bad values during training.')$/;\"","line":81},{"name":"input_fn","type":"f","filepath":"train.py","query":"/^def input_fn(is_training, data_dir, batch_size, num_epochs=1):$/;\"","line":176},{"name":"main","type":"f","filepath":"train.py","query":"/^def main(unused_argv):$/;\"","line":213},{"name":"parse_record","type":"f","filepath":"train.py","query":"/^def parse_record(raw_record):$/;\"","line":118},{"name":"parser","type":"v","filepath":"train.py","query":"/^parser = argparse.ArgumentParser()$/;\"","line":18},{"name":"preprocess_image","type":"f","filepath":"train.py","query":"/^def preprocess_image(image, label, is_training):$/;\"","line":153}],"utils/preprocessing.py":[{"name":"_B_MEAN","type":"v","filepath":"utils/preprocessing.py","query":"/^_B_MEAN = 103.94$/;\"","line":9},{"name":"_G_MEAN","type":"v","filepath":"utils/preprocessing.py","query":"/^_G_MEAN = 116.78$/;\"","line":8},{"name":"_R_MEAN","type":"v","filepath":"utils/preprocessing.py","query":"/^_R_MEAN = 123.68$/;\"","line":7},{"name":"_parse_function","type":"f","filepath":"utils/preprocessing.py","query":"/^  def _parse_function(filename, is_label):$/;\"","class":"function:eval_input_fn","line":226},{"name":"decode_labels","type":"f","filepath":"utils/preprocessing.py","query":"/^def decode_labels(mask, num_images=1, num_classes=21):$/;\"","line":23},{"name":"eval_input_fn","type":"f","filepath":"utils/preprocessing.py","query":"/^def eval_input_fn(image_filenames, label_filenames=None, batch_size=1):$/;\"","line":213},{"name":"label_colours","type":"v","filepath":"utils/preprocessing.py","query":"/^label_colours = [(0, 0, 0),  # 0=background$/;\"","line":12},{"name":"mean_image_addition","type":"f","filepath":"utils/preprocessing.py","query":"/^def mean_image_addition(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):$/;\"","line":49},{"name":"mean_image_subtraction","type":"f","filepath":"utils/preprocessing.py","query":"/^def mean_image_subtraction(image, means=(_R_MEAN, _G_MEAN, _B_MEAN)):$/;\"","line":82},{"name":"random_crop_or_pad_image_and_label","type":"f","filepath":"utils/preprocessing.py","query":"/^def random_crop_or_pad_image_and_label(image, label, crop_height, crop_width, ignore_label):$/;\"","line":156},{"name":"random_flip_left_right_image_and_label","type":"f","filepath":"utils/preprocessing.py","query":"/^def random_flip_left_right_image_and_label(image, label):$/;\"","line":194},{"name":"random_rescale_image_and_label","type":"f","filepath":"utils/preprocessing.py","query":"/^def random_rescale_image_and_label(image, label, min_scale, max_scale):$/;\"","line":115}],"evaluate.py":[{"name":"_NUM_CLASSES","type":"v","filepath":"evaluate.py","query":"/^_NUM_CLASSES = 21$/;\"","line":44},{"name":"choices","type":"v","filepath":"evaluate.py","query":"/^                    choices=['resnet_v2_50', 'resnet_v2_101'],$/;\"","line":37},{"name":"choices","type":"v","filepath":"evaluate.py","query":"/^                    choices=[8, 16],$/;\"","line":41},{"name":"compute_accuracy","type":"f","filepath":"evaluate.py","query":"/^    def compute_accuracy(total_cm):$/;\"","class":"function:main","line":126},{"name":"compute_mean_iou","type":"f","filepath":"evaluate.py","query":"/^    def compute_mean_iou(total_cm):$/;\"","class":"function:main","line":93},{"name":"help","type":"v","filepath":"evaluate.py","query":"/^                    help=\"Base directory for the model. \"$/;\"","line":32},{"name":"help","type":"v","filepath":"evaluate.py","query":"/^                    help='Output stride for DeepLab v3. Currently 8 or 16 is supported.')$/;\"","line":42},{"name":"help","type":"v","filepath":"evaluate.py","query":"/^                    help='Path to the file listing the evaluation images.')$/;\"","line":29},{"name":"help","type":"v","filepath":"evaluate.py","query":"/^                    help='The architecture of base Resnet building block.')$/;\"","line":38},{"name":"help","type":"v","filepath":"evaluate.py","query":"/^                    help='The directory containing the ground truth label data.')$/;\"","line":26},{"name":"help","type":"v","filepath":"evaluate.py","query":"/^                    help='The directory containing the image data.')$/;\"","line":23},{"name":"main","type":"f","filepath":"evaluate.py","query":"/^def main(unused_argv):$/;\"","line":47},{"name":"parser","type":"v","filepath":"evaluate.py","query":"/^parser = argparse.ArgumentParser()$/;\"","line":20}],"export_inference_graph.py":[{"name":"_NUM_CLASSES","type":"v","filepath":"export_inference_graph.py","query":"/^_NUM_CLASSES = 21$/;\"","line":36},{"name":"choices","type":"v","filepath":"export_inference_graph.py","query":"/^                    choices=['resnet_v2_50', 'resnet_v2_101'],$/;\"","line":28},{"name":"choices","type":"v","filepath":"export_inference_graph.py","query":"/^                    choices=[8, 16],$/;\"","line":32},{"name":"help","type":"v","filepath":"export_inference_graph.py","query":"/^                    help=\"Base directory for the model. \"$/;\"","line":20},{"name":"help","type":"v","filepath":"export_inference_graph.py","query":"/^                    help='Output stride for DeepLab v3. Currently 8 or 16 is supported.')$/;\"","line":33},{"name":"help","type":"v","filepath":"export_inference_graph.py","query":"/^                    help='The architecture of base Resnet building block.')$/;\"","line":29},{"name":"help","type":"v","filepath":"export_inference_graph.py","query":"/^                    help='The directory where the exported SavedModel will be stored.')$/;\"","line":25},{"name":"main","type":"f","filepath":"export_inference_graph.py","query":"/^def main(unused_argv):$/;\"","line":39},{"name":"parser","type":"v","filepath":"export_inference_graph.py","query":"/^parser = argparse.ArgumentParser()$/;\"","line":17},{"name":"serving_input_receiver_fn","type":"f","filepath":"export_inference_graph.py","query":"/^  def serving_input_receiver_fn():$/;\"","class":"function:main","line":56}],"inference.py":[{"name":"_NUM_CLASSES","type":"v","filepath":"inference.py","query":"/^_NUM_CLASSES = 21$/;\"","line":49},{"name":"choices","type":"v","filepath":"inference.py","query":"/^                    choices=['resnet_v2_50', 'resnet_v2_101'],$/;\"","line":39},{"name":"choices","type":"v","filepath":"inference.py","query":"/^                    choices=[8, 16],$/;\"","line":43},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help=\"Base directory for the model. \"$/;\"","line":34},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help='Output stride for DeepLab v3. Currently 8 or 16 is supported.')$/;\"","line":44},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help='Path to the directory to generate the inference results')$/;\"","line":28},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help='Path to the file listing the inferring images.')$/;\"","line":31},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help='The architecture of base Resnet building block.')$/;\"","line":40},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help='The directory containing the image data.')$/;\"","line":25},{"name":"help","type":"v","filepath":"inference.py","query":"/^                    help='Whether to use debugger to track down bad values during training.')$/;\"","line":47},{"name":"main","type":"f","filepath":"inference.py","query":"/^def main(unused_argv):$/;\"","line":52},{"name":"parser","type":"v","filepath":"inference.py","query":"/^parser = argparse.ArgumentParser()$/;\"","line":22}],"utils/dataset_util.py":[{"name":"bytes_feature","type":"f","filepath":"utils/dataset_util.py","query":"/^def bytes_feature(value):$/;\"","line":31},{"name":"bytes_list_feature","type":"f","filepath":"utils/dataset_util.py","query":"/^def bytes_list_feature(value):$/;\"","line":35},{"name":"float_list_feature","type":"f","filepath":"utils/dataset_util.py","query":"/^def float_list_feature(value):$/;\"","line":39},{"name":"int64_feature","type":"f","filepath":"utils/dataset_util.py","query":"/^def int64_feature(value):$/;\"","line":23},{"name":"int64_list_feature","type":"f","filepath":"utils/dataset_util.py","query":"/^def int64_list_feature(value):$/;\"","line":27},{"name":"make_initializable_iterator","type":"f","filepath":"utils/dataset_util.py","query":"/^def make_initializable_iterator(dataset):$/;\"","line":91},{"name":"read_dataset","type":"f","filepath":"utils/dataset_util.py","query":"/^def read_dataset($/;\"","line":108},{"name":"read_examples_list","type":"f","filepath":"utils/dataset_util.py","query":"/^def read_examples_list(path):$/;\"","line":43},{"name":"recursive_parse_xml_to_dict","type":"f","filepath":"utils/dataset_util.py","query":"/^def recursive_parse_xml_to_dict(xml):$/;\"","line":65}],"create_pascal_tf_record.py":[{"name":"create_tf_record","type":"f","filepath":"create_pascal_tf_record.py","query":"/^def create_tf_record(output_filename,$/;\"","line":84},{"name":"dict_to_tf_example","type":"f","filepath":"create_pascal_tf_record.py","query":"/^def dict_to_tf_example(image_path,$/;\"","line":38},{"name":"help","type":"v","filepath":"create_pascal_tf_record.py","query":"/^                    help='Path to the directory containing the PASCAL VOC data.')$/;\"","line":20},{"name":"help","type":"v","filepath":"create_pascal_tf_record.py","query":"/^                    help='Path to the directory to create TFRecords outputs.')$/;\"","line":23},{"name":"help","type":"v","filepath":"create_pascal_tf_record.py","query":"/^                    help='Path to the file listing the training data.')$/;\"","line":26},{"name":"help","type":"v","filepath":"create_pascal_tf_record.py","query":"/^                    help='Path to the file listing the validation data.')$/;\"","line":29},{"name":"help","type":"v","filepath":"create_pascal_tf_record.py","query":"/^                    help='The directory containing the augmented label data.')$/;\"","line":35},{"name":"help","type":"v","filepath":"create_pascal_tf_record.py","query":"/^                    help='The directory containing the image data.')$/;\"","line":32},{"name":"main","type":"f","filepath":"create_pascal_tf_record.py","query":"/^def main(unused_argv):$/;\"","line":119},{"name":"parser","type":"v","filepath":"create_pascal_tf_record.py","query":"/^parser = argparse.ArgumentParser()$/;\"","line":17}]}}